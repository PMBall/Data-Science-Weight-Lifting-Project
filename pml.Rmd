---
title: "Weight Lifting Classification Project"
author: "by Pablo Ballesteros"
output: html_document
---

###Summary

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.  
The objective of this study is to be able to predict not only if a particular weight lifting exercise is well done, but also to be able to predict the type of mistake done. This will be done using data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

The training data is available here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv  
The test data is available here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv  
Further information about the data can be found here: http://groupware.les.inf.puc-rio.br/har  

###Loading the data and the libraries

Read the train and testing variables

```{r,message=FALSE}
library(caret)
library(ggplot2)
trainval<-read.csv("pml-training.csv")
testing<-read.csv("pml-testing.csv") 
```

###Cleaning the data

We clean the data a bit, removing columns with a lot of missing values and the columns with time stamps and user info, which won't be useful for predicting.

```{r,message=FALSE}
#Take out any columns with more than 70% missing values
nacolumns<-which(apply(trainval,2,function(x) {mean(is.na(x) | x=="")})>.7)
seltrain<-trainval[,which(!(1:160 %in% nacolumns))]
#Take out the first columns that should not be used to make any predictions
seltrain<-seltrain[,-(1:7)]
```

Check if we have another non-interesting column using the nearZeroVar function.

```{r,message=FALSE}
nearZeroVar(seltrain)
```

###Splitting the data

We create a partition of the data, so we can have  a validation set. Given that we have a large amount of data, 80% of this is going to be used to build the model and the rest is going to be used for cross-validation.

```{r}
set.seed(3366)
srows<-createDataPartition(seltrain$classe,p=0.8)[[1]]
strain<-seltrain[srows,]
svalidation<-seltrain[-srows,]
```

###Building the model

To build the initial model a random forest model is used.

```{r rf,cache=TRUE,message=FALSE}
set.seed(6633)
fit1<-train(classe~.,data=strain,method="rf")
```

###Cross Validation and out-of-sample error

A confusion matrix is built and see how well it predicts the training set

```{r,message=FALSE}
confusionMatrix(predict(fit1),strain$classe)
```

The predictive value for the training set is 100%. The same model is used with the validation set to see the out-of-sample accuracy

```{r}
confusionMatrix(predict(fit1,newdata=svalidation),svalidation$classe)
```

The expected OOS accuracy is around 99.5% which is very good.

###Variable Importance

The most important variables are plotted.

```{r}
dat<-varImp(fit1)[[1]]
dat<-dat[order(dat$Overall,decreasing = TRUE),, drop = FALSE]
dat$measurement<-row.names(dat)
dat$measurement <- reorder(dat$measurement, -dat$Overall)
g<-ggplot(data=dat[1:10,],aes(x=measurement,y=Overall))
g<-g+geom_bar(stat = "identity", colour="blue", fill="dark blue")
g<-g+theme(axis.text.x = element_text(angle = 45, hjust = 1))
g + ggtitle("Variable Importance") + xlab("Measurement") + ylab("Importance")
```

###Prediction

As a last step, a prediction over the testing data is done.

```{r}
predict(fit1,newdata=testing)
```